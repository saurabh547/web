#Perform Spam Classifier. 
import matplotlib.pyplot as plt 
import csv 
import sklearn 
import pickle 
from wordcloud import WordCloud 
import pandas as pd 
import numpy as np 
import nltk 
from nltk.corpus import stopwords 
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer 
from sklearn.tree import DecisionTreeClassifier 
from sklearn.model_selection import GridSearchCV,train_test_split,StratifiedKFold,cross_val_score,learning_curve 
data=pd.read_csv("/content/drive/MyDrive/spam.csv",encoding="ISO-8859-1") 
data.head() 
data= data.drop(["Unnamed: 2", "Unnamed: 3", "Unnamed: 4"], axis=1) 
data= data.rename(columns={"v2" : "text", "v1":"label"}) 
data[1990:2000] 
data['label'].value_counts() 
# Import nltk packages and Punkt Tokenizer Models 
import nltk 
nltk.download("punkt") 
import warnings 
warnings.filterwarnings('ignore') 
ham_words = '' 
spam_words = '' 
# Creating a corpus of spam messages 
for val in data[data['label'] == 'spam'].text: text = val.lower() 
tokens = nltk.word_tokenize(text) 
for words in tokens: spam_words = spam_words + words + ' ' 
# Creating a corpus of ham messages 
for val in data[data['label'] == 'ham'].text: text = text.lower() 
tokens = nltk.word_tokenize(text) 
for words in tokens: ham_words = ham_words + words + ' ' 
spam_wordcloud = WordCloud(width=500, height=300).generate(spam_words) 
ham_wordcloud = WordCloud(width=500, height=300).generate(ham_words) 
#Spam Word cloud 
plt.figure( figsize=(10,8), facecolor='w') 
plt.imshow(spam_wordcloud) 
plt.axis("off") 
plt.tight_layout(pad=0) 
plt.show() 
#Creating Ham wordcloud 
plt.figure( figsize=(10,8), facecolor='g') 
plt.imshow(ham_wordcloud) 
plt.axis("off") 
plt.tight_layout(pad=0) 
plt.show() 
data = data.replace(['ham','spam'],[0, 1]) 
data.head(10) 
import nltk 
nltk.download('stopwords') 
#remove the punctuations and stopwords 
import string 
def text_process(text): 
 text = text.translate(str.maketrans('', '', string.punctuation)) 
 text = [word for word in text.split() if word.lower() not in stopwords.words('english')] 
 return " ".join(text) 
data['text'] = data['text'].apply(text_process) 
data.head() 
text = pd.DataFrame(data['text']) 
label = pd.DataFrame(data['label']) 
## Counting how many times a word appears in the dataset
from collections import Counter 
total_counts = Counter() 
for i in range(len(text)): 
 for word in text.values[i][0].split(" "): total_counts[word] += 1 
print("Total words in data set: ", len(total_counts)) 
# Sorting in decreasing order (Word with highest frequency appears first) 
vocab = sorted(total_counts, key=total_counts.get, reverse=True) 
print(vocab[:60]) 
# Mapping from words to index 
vocab_size = len(vocab) 
word2idx = {} 
#print vocab_size 
for i, word in enumerate(vocab): 
 word2idx[word] = i 
# Text to Vector 
def text_to_vector(text): 
 word_vector = np.zeros(vocab_size) 
 for word in text.split(" "): 
  if word2idx.get(word) is None: 
    continue 
 else: 
  word_vector[word2idx.get(word)] += 1 
  return np.array(word_vector) 
# # Convert all titles to vectors 
# word_vectors = np.zeros((len(text), len(vocab)), dtype=np.int_) 
# for i, text in enumerate(text.iterrows()): 
#  word_vectors[i] = text_to_vector(text_[0]) 
#  word_vectors.shape 
# # Convert all titles to vectors 
# word_vectors = np.zeros((len(text), len(vocab)), dtype=np.int_) 
# for i, text in enumerate(text.iterrows()):
#   word_vectors[i] = text_to_vector(text_[0]) 
#   word_vectors.shape 
# #convert the text data into vectors 
# from sklearn.feature_extraction.text import TfidfVectorizer 
# vectorizer = TfidfVectorizer() 
# vectors = vectorizer.fit_transform(data['text']) 
# vectors.shape 
# #features = word_vectors 
# features = vectors 
# #split the dataset into train and test set 
# X_train, X_test, y_train, y_test = train_test_split(features, data['label'], test_size=0.15, 
# random_state=111) 
# #import sklearn packages for building classifiers 
# from sklearn.linear_model import LogisticRegression 
# lrc = LogisticRegression(solver='liblinear', penalty='l1') 
# #create a dictionary of variables and models 
# clfs = {'LR': lrc} 
# #fit the data onto the models 
# def train(clf, features, targets): 
#  clf.fit(features, targets) 
# def predict(clf, features): 
#  return (clf.predict(features)) 
# from sklearn.metrics import accuracy_score 
# pred_scores_word_vectors = [] 
# for k,v in clfs.items(): 
#  train(v, X_train, y_train) 
#  pred = predict(v, X_test) 
#  pred_scores_word_vectors.append((k, [accuracy_score(y_test , pred)])) 
# pred_scores_word_vectors 
# #write functions to detect if the message is spam or not 
# def find(x): 
#  if x == 1: 
#   print ("Message is SPAM") 
#  else: 
#   print ("Message is NOT Spam") 
# newtext = ["Free entry"] 
# integers = vectorizer.transform(newtext) 
# x = lrc.predict(integers) 
# find(x)
